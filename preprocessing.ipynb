{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finefood import preprocessing, utils\n",
    "from keras.utils import np_utils\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from finefood import preprocessing, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/Reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path).set_index(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = df['Text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "1         bought sever vital can dog food product found ...\n",
       "2         product arriv label jumbo salt peanut the pean...\n",
       "3         confect around centuri light pillowi citrus ge...\n",
       "4         look secret ingredi robitussin believ found it...\n",
       "5         great taffi great price wide assort yummi taff...\n",
       "6         got wild hair taffi order five pound bag taffi...\n",
       "7         saltwat taffi great flavor soft chewi candi in...\n",
       "8         taffi good soft chewi flavor amaz would defini...\n",
       "9         right i am most sprout cat eat grass love it r...\n",
       "10        healthi dog food good digest also good small p...\n",
       "11        know cactus tequila uniqu combin ingredi flavo...\n",
       "12        one boy need lose weight did not put food floo...\n",
       "13        cat happili eat felida platinum two year got n...\n",
       "14        good flavor ! came secur pack fresh delici ! l...\n",
       "15        strawberri twizzler guilti pleasur yummi six p...\n",
       "16        daughter love twizzler shipment six pound real...\n",
       "17        love eat good watch look movi ! sweet like tra...\n",
       "18        satisfi twizzler purchas share other enjoy the...\n",
       "19        twizzler strawberri childhood favorit candi ma...\n",
       "20        candi deliv fast purchas reason price home bou...\n",
       "21        husband twizzler addict we have bought mani ti...\n",
       "22        bought husband current oversea love these appa...\n",
       "23        rememb buy candi kid qualiti drop year still s...\n",
       "24        love candi weight watcher cut back still crave it\n",
       "25        live yrs now miss twizzler ! ! back visit some...\n",
       "26        product receiv advertis <br <br <a href = http...\n",
       "27              candi red flavor plan chewi would never buy\n",
       "28        glad amazon carri batteri hard time find elsew...\n",
       "29        got mum diabet need watch sugar intak father s...\n",
       "30        know cactus tequila uniqu combin ingredi flavo...\n",
       "                                ...                        \n",
       "568425    i have tri sever violet flavor candi past favo...\n",
       "568426    candi good flavor quit unlik anyth common avai...\n",
       "568427    candi tasti total scam price jar size soda can...\n",
       "568428    look violet candi perfect balanc sweet floral ...\n",
       "568429    pricey enjoy treat flavor wonderful packag nic...\n",
       "568430    candi mild flavor compar choward violet candi ...\n",
       "568431    product bit pricey amt receiv but i want candi...\n",
       "568432    definit worth buy flavor water teaspoon bean r...\n",
       "568433    thought soup would like chilli wateri could to...\n",
       "568434    bought soup today local grocer brought back wo...\n",
       "568435    soup most broth although kick it also sweet ta...\n",
       "568436    most broth advertis 3 4 cup veget howev cup co...\n",
       "568437    past would buy larg quantiti baker ammonia wou...\n",
       "568438    ammonium bicarbon nice littl packag need ammon...\n",
       "568439    ever use ammonium bicarbon bake tri it loft so...\n",
       "568440    need recip wife interest tri howev we<br have ...\n",
       "568441    indi candi gummi absolut delici buy kid ca not...\n",
       "568442    quick easi ! similar gulasch guest hous german...\n",
       "568443    product great give much energi tast great tri ...\n",
       "568444    love tea first discov pleasur chai attend yoga...\n",
       "568445    foodi use lot chines spice powder daili cook i...\n",
       "568446    make mix yourself star anis often difficult fi...\n",
       "568447    order month back great latest batch terribl an...\n",
       "568448    hope msg this tast extrem good make toast fish...\n",
       "568449    complaint there much it use huge amount spice ...\n",
       "568450    great sesam chicken this good better restur ea...\n",
       "568451    i am disappoint flavor chocol note especi weak...\n",
       "568452    star small give 10 - 15 one train session tri ...\n",
       "568453    best treat train reward dog good groom lower c...\n",
       "568454    satisfi product advertis use cereal raw vinega...\n",
       "Name: Text, Length: 568454, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92578 unique tokens.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MAX_SEQUENCE_LENGTH' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-daf50ad5d557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s unique tokens.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of data tensor:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_SEQUENCE_LENGTH' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(clean_texts)\n",
    "sequences = tokenizer.texts_to_sequences(clean_texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(df.Score))[:, 1:]\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, word_to_index, word_to_vec_map = utils.read_glove_vecs(\"./data/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((MAX_NUM_WORDS, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > MAX_NUM_WORDS - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = word_to_vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"./data/Reviews.h5\", \"w\") as fp:\n",
    "    fp.create_dataset(\"text_indices\", data=data,  compression=\"gzip\")\n",
    "    fp[\"scores_oh\"] = labels\n",
    "    fp.create_dataset(\"embedding_matrix\", data=embedding_matrix,  compression=\"gzip\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5: 363122, 1: 52268, 4: 80655, 2: 29769, 3: 42640})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df.Score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Score.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6387887146541321"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "363122/568454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
